import polars as pl
import sys

# --- Configuration ---

# The CSV file generated by your scraping script
INPUT_CSV_FILE = "scraped_links_async.csv"

# The name of the final, filtered output file
OUTPUT_CSV_FILE = "filtered_download_links.csv"

# --- The Ignore List ---
# Add any domain or URL part you want to exclude from the final list.
# The script will remove any link that CONTAINS any of these strings.
# This is case-insensitive.
IGNORE_LIST = [
    # Blog's own links & related infrastructure
    "elrincondelkitsune.blogspot.com",
    "blogger.com",
    "bp.blogspot.com",  # Blogspot's image hosting CDN
    "blogger.googleusercontent.com", # Blogger's hosted content/images
    "/p/indice.html", # Specific internal blog page

    # Chat & Community
    "chatdelkitsune.chatango.com", # Your chat link

    # Licenses & Legal
    "creativecommons.org", # Creative Commons licenses

    # Common services not related to downloads
    "google.com",  # Catches search, drive, etc.
    "youtube.com",
    "youtu.be",    # YouTube's short-link
    "disqus.com",  # Commenting system
    "gravatar.com",
    "wikipedia.org",
    "archive.org", # Often for preservation, not direct downloads
    "twitter.com",
    "facebook.com",
    "ask.fm",
    "images.wikia.com",

    # Image hosts that are usually just for viewing
    "imgur.com",
    "postimg.cc",
    "ibb.co",
    "photobucket.com",
    "deviantart.com",

    # Link shorteners/monetizers (often lead to ads, not direct content)
    "adf.ly",
    "bc.vc",
    "sh.st",
    "bit.ly",

    # Specific website FAQs or non-download pages
    "vocaloid.ga", # Specific FAQ page
    "vocaloid.com"
]

def filter_scraped_links() -> None:
    """
    Reads the raw scraped CSV, filters out unwanted links using an ignore list,
    removes duplicates, and writes the clean data to a new CSV file using Polars.
    """
    print("--- Starting Link Filtering Process with Polars ---")

    # Construct a single regex pattern from the ignore list.
    # The `|` acts as an "OR", so it will match any link containing any of the items.
    # The `(?i)` flag makes the search case-insensitive.
    ignore_pattern = f"(?i)({'|'.join(IGNORE_LIST)})"
    print(f"[*] Using {len(IGNORE_LIST)} patterns to filter links.")

    try:
        # 1. Read the initial CSV file
        df = pl.read_csv(INPUT_CSV_FILE)
        print(f"[+] Read {df.height} rows from '{INPUT_CSV_FILE}'.")

        # 2. Reshape the data: The 'Links' column is one long string.
        #    - Split the 'Links' string by newline characters into a list of strings.
        #    - Use `explode` to create a new row for each item in the list.
        #    This transforms the DataFrame from a "wide" to a "long" format.
        df_long = (
            df.with_columns(
                pl.col("Links").str.split("\n")
            )
            .explode("Links")
            .rename({"Links": "found_link"}) # Rename for clarity
        )

        # Remove any empty rows that might result from the split
        df_cleaned = df_long.filter(pl.col("found_link") != "")
        initial_link_count = df_cleaned.height
        print(f"[*] Exploded data into {initial_link_count} individual, non-empty links.")

        # 3. Filter the links based on rules
        #    Step 3a: Remove links that are just a single forward slash "/"
        #    This is done before the more complex regex matching for efficiency.
        df_cleaned = df_cleaned.filter(pl.col("found_link") != "/")

        #    Step 3b: Use `str.contains` with the regex pattern to find unwanted links.
        #    The `~` operator negates the filter, so we KEEP rows that DON'T match.
        filtered_df = df_cleaned.filter(
            ~pl.col("found_link").str.contains(ignore_pattern)
        )
        count_after_rules = filtered_df.height
        removed_by_rules = initial_link_count - count_after_rules
        print(f"[*] Removed {removed_by_rules} links based on the ignore list and other rules.")


        # 4. Remove duplicate links
        #    Now that we have our clean list, we remove any exact duplicates.
        #    The `.unique()` method is highly optimized for this task.
        deduplicated_df = filtered_df.unique(subset=["found_link"])
        final_link_count = deduplicated_df.height
        duplicates_removed = count_after_rules - final_link_count
        print(f"[*] Removed {duplicates_removed} duplicate links.")

        # 5. Write the final, clean data to a new CSV
        deduplicated_df.write_csv(OUTPUT_CSV_FILE)

        # --- Final summary message ---
        print("\n" + "="*50)
        print("FILTERING COMPLETE")
        print("="*50 + "\n")
        print(f"[+] Initial non-empty links found: {initial_link_count}")
        print(f"[+] Links removed by ignore list/rules: {removed_by_rules}")
        print(f"[+] Duplicate links removed: {duplicates_removed}")
        print("--------------------------------------------------")
        print(f"[+] Final unique download links kept: {final_link_count}")
        print(f"\n[SUCCESS] Filtered data saved to '{OUTPUT_CSV_FILE}'")

    except FileNotFoundError:
        print(f"\n[ERROR] The input file '{INPUT_CSV_FILE}' was not found.")
        print("Please run your scraping script first to generate it.")
        sys.exit(1) # Exit with an error code
    except Exception as e:
        print(f"\n[ERROR] An unexpected error occurred: {e}")
        sys.exit(1)


if __name__ == "__main__":
    filter_scraped_links()